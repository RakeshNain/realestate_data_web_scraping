{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using python collecting real-world data by web scrapping realestate website and doing data wrangling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__contributed by:__\n",
    ">Name: **Rakesh Nain**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In any data science project one of the most asked questions is how to get the data, where is the data. I would say there is plenty of data around you, you just need to extract it. For example, on the internet, there are millions of petabytes of data available and most of it is free. All you need to know is how to extract it and make it useful for your organisation. I would say any type of organisation can make use of the free data available on the internet for their business gains. They can use web scraping to extract it.\n",
    "\n",
    "For demonstrating web scrapping in this article I will be scraping data from domian.com. Domain.com is a real estate website. I will be scrapping the price, number of bedrooms, number of bathrooms, number of parking, address and location(latitude and longitude) of each house in Melbourne, Australia.\n",
    "\n",
    "Before diving into python programming you need to know some basics about html.\n",
    "All the web pages are written in HTML(Hyper Text Markup Language).\n",
    "HTML is the standard markup language for creating Web pages\n",
    "HTML describes the structure of a Web page\n",
    "HTML elements tell the browser how to display the content\n",
    "HTML elements label pieces of content such as \"this is a heading\", \"this is a paragraph\", \"this is a link\", etc.\n",
    "\n",
    "A simple HTML document looks like this:\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "<title>Page Title</title>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<h1>This is a Heading</h1>\n",
    "<p>This is a paragraph.</p>\n",
    "\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "Where,\n",
    "- The <!DOCTYPE html> declaration defines that this document is an HTML5 document\n",
    "- The `<html>` element is the root element of an HTML page\n",
    "- The `<head>` element contains meta information about the HTML page\n",
    "- The `<title>` element specifies a title for the HTML page (which is shown in the browser's title bar or in the page's tab)\n",
    "- The `<body>` element defines the document's body, and is a container for all the visible contents, such as headings, paragraphs, images, hyperlinks, tables, lists, etc.\n",
    "- The `<h1>` element defines a large heading\n",
    "- The `<p>` element defines a paragraph\n",
    "\n",
    "You can get this HTML document of any website by doing a right-click on a webpage and then selecting \"View page source\"(available in Microsoft Edge and Google Chrome). All the content on the webpage will be inside this HTML document in a well-structured format, all you need to do is extract the required data from this HTML document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. __Data Collection__\n",
    "\n",
    "    There are various libraries available in Python to get this HTML document and parse it into the required format you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample code to get a HTML document and parse it into the required format you want\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen(\"https://www.domain.com.au/sale/melbourne-region-vic/\")\n",
    "bsobj = BeautifulSoup(html, \"lxml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above code, urlopen is extracting HTML document of given web page and BeautifulSoup is phrasing it into lxml format. lxml format is easy to understand, you can use another format you want such as json etc.\n",
    "\n",
    "<img src=\"Screenshot (358).png\" alt=\"Search result for melbourne houses on Domain.com\" title=\"Domain.com\" width=\"600\" />\n",
    "\n",
    "Search result for melbourne houses on Domain.comThe above screenshot showing \"https://www.domain.com.au/sale/melbourne-region-vic/\" URL result and it is showing all the properties available for selling in Melbourne but we need to find webpages for each Melbourne properties available on \"https://www.domain.com.au/sale/melbourne-region-vic/\"(this search result page). We can do it by extracting all the URLs available on this page and storing them into a list. One more thing to add here is that there are 50 pages for Melbourne house search on Domain.com and this is only 1st page so we need to go to every 50 pages and extract all the URLs for each advertised house in Melbourne. We need to apply the loop for 50 iterations, each iteration for each page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "# home url of domian.com australia\n",
    "home_url = \"https://www.domain.com.au\"\n",
    "# number of pages of search result are 50, so we need to \n",
    "page_numbers = list(range(50))[1:50]\n",
    "# list to store all the urls of properties\n",
    "list_of_links = []\n",
    "# for loop for all 50 search(melbourne region) pages\n",
    "for page in page_numbers:\n",
    " \n",
    "    # extracting html document of search page\n",
    "    html = urlopen(home_url + \"/sale/melbourne-region-vic/?sort=price-desc&page=\" + str(page))\n",
    "    # parsing html document to 'lxml' format\n",
    "    bsobj = BeautifulSoup(html, \"lxml\")\n",
    "    # finding all the links available in 'ul' tag whos 'data-testid' is 'results'\n",
    "    all_links = bsobj.find(\"ul\", {\"data-testid\": \"results\"}).findAll(\"a\", href=re.compile(\"https://www.domain.com.au/*\"))\n",
    "# inner loop to find links inside each property page because few properties are project so they have more properties inside their project page\n",
    "    for link1 in all_links:\n",
    "        # checking if it is a project and then performing similar thing I did above\n",
    "        if 'project' in link1.attrs['href']:\n",
    "            inner1_html = urlopen(link1.attrs['href'])\n",
    "            inner1_bsobj = BeautifulSoup(inner1_html, \"lxml\")\n",
    "            for link2 in inner1_bsobj.find(\"div\", {\"name\": \"listing-details__other-listings\"}).findAll(\"a\", href=re.compile(\"https://www.domain.com.au/*\")):\n",
    "                if 'href' in link2.attrs:\n",
    "                    list_of_links.append(link2.attrs['href'])\n",
    "        else:\n",
    "            list_of_links.append(link1.attrs['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can simply copy and paste the above code and do some modifications according to your needs, and try to run. Or you can download my jupyter notebook.\n",
    "\n",
    "Above I did a few different things:\n",
    "- I used the search page which is sorted by price. I did this so that it will be easier to impute missing price of houses. I will explain it more in data wrangling part below.\n",
    "- The Inner loop is used because few properties are not just properties, they are projects and each project have more properties URL links inside their page.\n",
    "\n",
    "Now we have all the URLs of each property in Melbourne, Australia. Each URL is unique for each property in Melbourne. Our next step will be, go inside each URL and extract price, number of bedrooms, number of bathrooms, number of parking, address and location(latitude and longitude)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing duplicate links while maintaining the order of urls\n",
    "abc_links = [] \n",
    "for i in list_of_links: \n",
    "    if i not in abc_links: \n",
    "        abc_links.append(i) \n",
    "        \n",
    "# defining required regural expression for data extraction     \n",
    "pattern = re.compile(r'>(.+)<!.*>(.+?)</span>.*')\n",
    "pattern1 = re.compile(r'>(.+)<.')\n",
    "pattern2 = re.compile(r'destination=(.+)\" rel=.')\n",
    "basic_feature_list = []\n",
    "# loop to iterate through each url\n",
    "for link in abc_links:\n",
    "    \n",
    "    # opening urls\n",
    "    html = urlopen(link)\n",
    "    \n",
    "    # converting html document to 'lxml' format\n",
    "    bsobj = BeautifulSoup(html, \"lxml\")\n",
    "    \n",
    "    # extracting address/name of property\n",
    "    property_name = bsobj.find(\"h1\", {\"class\": \"css-164r41r\"})\n",
    "    \n",
    "    # extracting baths, rooms, parking etc\n",
    "    all_basic_features = bsobj.find(\"div\", {\"class\": \"listing-details__listing-summary-features css-er59q5\"}).findAll(\"span\", {\"data-testid\": \"property-features-text-container\"})\n",
    "    \n",
    "    # extracting property price\n",
    "    property_price = bsobj.find(\"div\", {\"data-testid\": \"listing-details__summary-title\"})\n",
    "    \n",
    "    # extracting latitudes and longitudes\n",
    "    lat_long = bsobj.find(\"a\", {\"target\": \"_blank\", 'rel': \"noopener noreferer\"})\n",
    "    \n",
    "    # dictionary to store temporary data\n",
    "    basic_feature_dict = {}\n",
    "    \n",
    "    # few properties does not contain all the 4 features such as rooms, baths, parkings, area. So need to check\n",
    "    # how many features they contain\n",
    "    if len(all_basic_features) == 4:\n",
    "        basic_feature_dict[pattern.findall(str(all_basic_features[0]))[0][1]] = pattern.findall(str(all_basic_features[0]))[0][0]\n",
    "        basic_feature_dict[pattern.findall(str(all_basic_features[1]))[0][1]] = pattern.findall(str(all_basic_features[1]))[0][0]\n",
    "        basic_feature_dict[pattern.findall(str(all_basic_features[2]))[0][1]] = pattern.findall(str(all_basic_features[2]))[0][0]\n",
    "        basic_feature_dict[pattern.findall(str(all_basic_features[3]))[0][1]] = pattern.findall(str(all_basic_features[3]))[0][0]\n",
    "        \n",
    "    elif len(all_basic_features) == 3:\n",
    "        basic_feature_dict[pattern.findall(str(all_basic_features[0]))[0][1]] = pattern.findall(str(all_basic_features[0]))[0][0]\n",
    "        basic_feature_dict[pattern.findall(str(all_basic_features[1]))[0][1]] = pattern.findall(str(all_basic_features[1]))[0][0]\n",
    "        basic_feature_dict[pattern.findall(str(all_basic_features[2]))[0][1]] = pattern.findall(str(all_basic_features[2]))[0][0]\n",
    "        \n",
    "    elif len(all_basic_features) == 2:\n",
    "        basic_feature_dict[pattern.findall(str(all_basic_features[0]))[0][1]] = pattern.findall(str(all_basic_features[0]))[0][0]\n",
    "        basic_feature_dict[pattern.findall(str(all_basic_features[1]))[0][1]] = pattern.findall(str(all_basic_features[1]))[0][0]\n",
    "        \n",
    "    elif len(all_basic_features) == 1:\n",
    "        basic_feature_dict[pattern.findall(str(all_basic_features[0]))[0][1]] = pattern.findall(str(all_basic_features[0]))[0][0]\n",
    "# putting 'none' if price is missing    \n",
    "    if property_price is None:\n",
    "        basic_feature_dict['price'] = None\n",
    "        \n",
    "    else:\n",
    "        basic_feature_dict['price'] = pattern1.findall(str(property_price))[0]\n",
    "        \n",
    "    # putting 'none' if property name/address is missing       \n",
    "    if property_name is None:\n",
    "        basic_feature_dict['name'] = None\n",
    "        \n",
    "    else:\n",
    "        basic_feature_dict['name'] = pattern1.findall(str(property_name))[0]\n",
    "        \n",
    "    # putting 'none' if latitude and logitude are missing        \n",
    "    if lat_long is None:\n",
    "        basic_feature_dict['lat'] = None\n",
    "        basic_feature_dict['long'] = None\n",
    "        \n",
    "    else:\n",
    "        basic_feature_dict['lat'] = pattern2.findall(str(lat_long))[0].split(',')[0]\n",
    "        basic_feature_dict['long'] = pattern2.findall(str(lat_long))[0].split(',')[1]\n",
    "# appending all the data into a list\n",
    "    basic_feature_list.append(basic_feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the output of the above code gives us a list of dictionaries with all the available extracted data. In the below code we will be converting it into many individual lists because we have to do little more cleaning and extraction of above-extracted data and it will be easier to do in lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating empty lists\n",
    "beds_list = []\n",
    "baths_list = []\n",
    "parking_list = []\n",
    "area_list = []\n",
    "name_list = []\n",
    "lat_list = []\n",
    "long_list = []\n",
    "price_list = []\n",
    "# interating through list created above with data\n",
    "for row in basic_feature_list:\n",
    "    \n",
    "    # checking if the row cointains 'Beds', 'Bed' or nothing\n",
    "    if 'Beds' in row:\n",
    "        beds_list.append(row['Beds'])\n",
    "    elif 'bed' in row:\n",
    "        beds_list.append(row['Bed'])\n",
    "    else:\n",
    "        beds_list.append(None)\n",
    "        \n",
    "    # checking if the row cointains 'Baths', 'Bath' or nothing    \n",
    "    if 'Baths' in row:\n",
    "        baths_list.append(row['Baths'])\n",
    "    elif 'Bath ' in row:\n",
    "        baths_list.append(row['Bath'])\n",
    "    else:\n",
    "        baths_list.append(None)\n",
    "        \n",
    "    # checking if the row cointains 'Parking', '-' or nothing     \n",
    "    if 'Parking' in row and row['Parking'] != '−':\n",
    "        parking_list.append(row['Parking'])\n",
    "    else:\n",
    "        parking_list.append(None)\n",
    "        \n",
    "    # checking if the row cointains ' ', or nothing. Because empty space (i.e. ' ') reprsents area  \n",
    "    if ' ' in row:\n",
    "        area_list.append(row[' '])\n",
    "    else:\n",
    "        area_list.append(None)\n",
    "# checking if the row cointains 'name' that is address of property         \n",
    "    if 'name' in row:\n",
    "        name_list.append(row['name'])\n",
    "    else:\n",
    "        name_list.append(None)\n",
    "    \n",
    "    # checking if the row cointains 'price'         \n",
    "    if 'price' in row:\n",
    "        price_list.append(row['price'])\n",
    "    else:\n",
    "        price_list.append(None)        \n",
    "    \n",
    "    # checking if the row cointains 'lat' that is lattitude of property         \n",
    "    if 'lat' in row:\n",
    "        lat_list.append(row['lat'])\n",
    "    else:\n",
    "        lat_list.append(None)  \n",
    "        \n",
    "    # checking if the row cointains 'long' that is lattitude of property             \n",
    "    if 'long' in row:\n",
    "        long_list.append(row['long'])\n",
    "    else:\n",
    "        long_list.append(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have all the data in list format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. __Data Wrangling__\n",
    "\n",
    "    Few people do not want to show the price of the property so they do not put price in their property advertisement. Some times they do not put anything in the price column and sometimes they put something like 'contact dealer' or 'after inspection price' or something else. And also some people do not put directly price they put a range of price or price with some extra text before the price or after the price or both. So we need to handle all these situations and extract only the price and if the price is not given then put 'none' their. Below code is doing the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# creating a new empty price list\n",
    "actual_price_list = []\n",
    "# defining some regural expressions, they will be used to extract price of properties\n",
    "pattern1 = re.compile(r'\\$\\s?([0-9,\\.]+).*\\s?.+\\s?\\$\\s?([0-9,\\.]+)')\n",
    "pattern2 = re.compile(r'\\$([0-9,\\.]+)')\n",
    "# interating through price_list\n",
    "for i in range(len(price_list)):\n",
    "    \n",
    "    # check that a price is given or range of price is given\n",
    "    \n",
    "    if str(price_list[i]).count('$') == 1:\n",
    "        b_num = pattern2.findall(str(price_list[i]))\n",
    "        \n",
    "        # checking length of string, if it is less than or equal to 5 then price is in millions so need to convert the price\n",
    "        if len(b_num[0].replace(',', '')) > 5:\n",
    "            actual_price_list.append(float(b_num[0].replace(',', '')))\n",
    "        else:\n",
    "            actual_price_list.append(float(b_num[0].replace(',', ''))*1000000)\n",
    "        \n",
    "    elif str(price_list[i]).count('$') == 2:\n",
    "        a_num = pattern1.findall(str(price_list[i]))\n",
    "        random_error = random.randint(0, 10000)\n",
    "        \n",
    "        # checking length of string, if it is less than or equal to 5 then price is in millions so need to convert the price\n",
    "        if len(a_num[0][0].replace(',', '')) > 5 and len(a_num[0][1].replace(',', '')) > 5:\n",
    "            \n",
    "            # to take average price between two price range given\n",
    "            avg_price = (float(a_num[0][0].replace(',', '')) + float(a_num[0][1].replace(',','')))/2\n",
    "        else:\n",
    "            avg_price = (float(a_num[0][0].replace(',', '')) + float(a_num[0][1].replace(',',''))/2)*1000000\n",
    "            \n",
    "        # adding or subtracting the amount from the average price by normally distributed generated random number\n",
    "        avg_price = avg_price + random_error\n",
    "        actual_price_list.append(avg_price)\n",
    "    else:\n",
    "        actual_price_list.append('middle_price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are alot of missing values in price because many people do not want to give or show house price on the website. Now we need to impute the missing price and I came up with a trick.\n",
    "\n",
    "**The trick is that we sort houses by their price then all the houses with or without the shown price will be sorted. The sorting by the website is done using price given by the owners of the houses to the website but it is not shown on the website for users. This is why we extracted houses data from the website when website results are sorted by price.**\n",
    "\n",
    "Let us understand it using an example, suppose there are 10 houses and price of two houses is missing but we can sort houses according to their price, so 1st we sort them according to their price then we see that price of house 4 and house 5 is missing so we will take mean of the price of house 3 and house 6. And then impute missing prices with that mean value. Similar kind of thing we will be doing in the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to impute missing values at the start of list, because here we cannot take mean\n",
    "for i in range(len(actual_price_list)):\n",
    "    if actual_price_list[i] != 'middle_price':\n",
    "        for a in range(i, -1, -1):\n",
    "            actual_price_list[a] = actual_price_list[i]\n",
    "        break\n",
    "# here we will be taking mean and then add random number with same standard deviation normal distribution and then impute it\n",
    "for i in range(len(actual_price_list)):\n",
    "    if actual_price_list[i] == 'middle_price':\n",
    "        for j in range(i, len(actual_price_list)):\n",
    "            if actual_price_list[j] != 'middle_price':\n",
    "                mid = (actual_price_list[i-1] + actual_price_list[j])/2\n",
    "                if actual_price_list[j] > 12000000:\n",
    "                    for k in range(i, j):\n",
    "                        random_error = random.randint(-1000000, 1000000)\n",
    "                        mid = mid + random_error\n",
    "                        actual_price_list[k] = mid\n",
    "                    i = j\n",
    "                    break\n",
    "                elif actual_price_list[j] > 5000000:\n",
    "                    for k in range(i, j):\n",
    "                        random_error = random.randint(-100000, 100000)\n",
    "                        mid = mid + random_error\n",
    "                        actual_price_list[k] = mid\n",
    "                    i = j\n",
    "                    break\n",
    "                else:\n",
    "                    for k in range(i, j):\n",
    "                        random_error = random.randint(-10000, 10000)\n",
    "                        mid = mid + random_error\n",
    "                        actual_price_list[k] = mid\n",
    "                    i = j\n",
    "                    break\n",
    "            elif j == len(actual_price_list)-1:\n",
    "                for n in range(i, len(actual_price_list)):\n",
    "                    random_error = random.randint(-1000, 1000)\n",
    "                    a_price = actual_price_list[i-1]\n",
    "                    a_price = a_price + random_error\n",
    "                    actual_price_list[n] = a_price\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1054 entries, 0 to 1053\n",
      "Data columns (total 8 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   Beds       798 non-null    object \n",
      " 1   Baths      755 non-null    object \n",
      " 2   Parking    764 non-null    object \n",
      " 3   Area       627 non-null    object \n",
      " 4   Address    1054 non-null   object \n",
      " 5   Latitude   1054 non-null   object \n",
      " 6   Longitude  1054 non-null   object \n",
      " 7   Price      1054 non-null   float64\n",
      "dtypes: float64(1), object(7)\n",
      "memory usage: 37.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "house_dict = {}\n",
    "house_dict['Beds'] = beds_list\n",
    "house_dict['Baths'] = baths_list\n",
    "house_dict['Parking'] = parking_list\n",
    "house_dict['Area'] = area_list\n",
    "house_dict['Address'] = name_list\n",
    "house_dict['Latitude'] = lat_list\n",
    "house_dict['Longitude'] = long_list\n",
    "house_dict['Price'] = actual_price_list\n",
    "house_df = pd.DataFrame(house_dict)\n",
    "house_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'area' column have many null values which cannot be imputed so we will be deleting 'area' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df.drop('Area', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also converting beds, baths, parking string type into numeric type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_df[\"Beds\"] = pd.to_numeric(house_df[\"Beds\"])\n",
    "house_df[\"Baths\"] = pd.to_numeric(house_df[\"Baths\"])\n",
    "house_df[\"Parking\"] = pd.to_numeric(house_df[\"Parking\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform some explanatory data analytics to find problems in the data and then solve those issues. For example use scatter plot to check outliers in the data or use histogram to see the distribution of data etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Beds', ylabel='Frequency'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# scatter plot\n",
    "house_df.plot.scatter(x='Beds',\n",
    "                      y='Baths')\n",
    "# histogram\n",
    "house_df[\"Price\"].plot.hist(bins = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data cleansing is an iterative process. The first step of the cleansing process is data auditing. In this step, we identify the types of anomalies that reduce the data quality.  Data auditing is about programmatically checking the data using some validation rules that are pre-specified, and then creating a report of the quality of the data and its problems. We often apply some statistical tests in this step for examining the data.\n",
    "Data Anomalies can be classified at a high level into three categories:\n",
    "\n",
    "1. **Syntactic Anomalies**: \n",
    "describe characteristics concerning the format and values used for representation of the entities. Syntactic anomalies such as: lexical errors, domain format errors, syntactical error and irregularities.\n",
    "2. **Semantic Anomalies**: \n",
    "hinder the data collection from being a comprehensive and non-redundant representation of the mini-world. These types of anomalies include: Integrity constraint violations, contradictions, duplicates and invalid tuples\n",
    "3. **Coverage Anomalies**: \n",
    "decrease the amount of entities and entity properties from the mini-world that are represented in the data collection. Coverage anomalies are categorized as: missing values and missing tuples\n",
    "\n",
    "We give examples in this part of the auditing process that is applied to discover different anomalies in data.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Missing values__\n",
    "\n",
    "Source of missing values:\n",
    "\n",
    "* Data Extraction: It is possible that there are problems with extraction process. In such cases, we should double-check for correct data with data guardians. Some hashing procedures can also be used to make sure that data extraction is correct. Errors at data extraction stage are typically easy to find and can be corrected easily as well.\n",
    "* Data collection: These errors occur at time of data collection and are harder to correct. They can be categorized in four types:\n",
    ">* Missing completely at random: This is a case when the probability of missing variable is same for all observations. For example: respondents of data collection process decide that they will declare their earning after tossing a fair coin. If an head occurs, respondent declares his / her earnings & vice versa. Here each observation has equal chance of missing value.\n",
    ">* Missing at random: This is a case when variable is missing at random and missing ratio varies for different values / level of other input variables. For example: We are collecting data for age and female has higher missing value compare to male.\n",
    ">* Missing that depends on unobserved predictors: This is a case when the missing values are not random and are related to the unobserved input variable. For example: In a medical study, if a particular diagnostic causes discomfort, then there is higher chance of drop out from the study. This missing value is not at random unless we have included “discomfort” as an input variable for all patients.\n",
    ">* Missing that depends on the missing value itself: This is a case when the probability of missing value is directly correlated with missing value itself. For example: People with higher or lower income are likely to provide non-response to their earning.\n",
    "\n",
    "According to the research and after performing EDA(explanatory data analytics) you can safely find out what type of missingness in your data. Our extracted data is Missing completely at random and data set is huge so I have deleted all the rows with any 'none' value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-232efbd8dd03>:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cleaned_house_df['distance_to_city'] = dis_to_city\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "cleaned_house_df = house_df.dropna(how='any')\n",
    "\n",
    "cleaned_house_df.reset_index(drop = True, inplace = True)\n",
    "# radius of earth is 6378\n",
    "r = 6378\n",
    "dis_to_city = []\n",
    "for i in range(len(cleaned_house_df)):\n",
    "    \n",
    "    lat1_n = math.radians(-37.818078)\n",
    "    lat2 = math.radians(float(cleaned_house_df['Latitude'][i]))\n",
    "    \n",
    "    lon1_n = math.radians(144.96681)\n",
    "    lon2 = math.radians(float(cleaned_house_df['Longitude'][i]))\n",
    "    \n",
    "    lon_diff_n = lon2 - lon1_n\n",
    "    lat_diff_n = lat2 - lat1_n\n",
    "    \n",
    "    a_n = math.sin(lat_diff_n / 2)**2 + math.cos(lat1_n) * math.cos(lat2) * math.sin(lon_diff_n / 2)**2\n",
    "    c_n = 2 * math.atan2(math.sqrt(a_n), math.sqrt(1 - a_n))\n",
    "    \n",
    "    dis_to_city.append(round(r*c_n, 4))\n",
    "    \n",
    "cleaned_house_df['distance_to_city'] = dis_to_city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final step is to export Dataframe to any other tabular format file like excel file or CSV file etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting to csv file\n",
    "cleaned_house_df.to_csv('real_estate_data_csv.csv', index=False)\n",
    "# exporting to excel file\n",
    "cleaned_house_df.to_excel('real_estate_data_excel.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
